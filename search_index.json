[["index.html", "Introduction to Statistical Learning Using R Book Club Welcome", " Introduction to Statistical Learning Using R Book Club The R4DS Online Learning Community 2021-10-12 Welcome This is a companion for the book Introduction to Statistical Learning Using R by Gareth James, Daniela Witten, Trevor Hastie, and Rob Tibshirani (Springer Science+Business Media, LLC, part of Springer Nature, copyright 2021, 978-1-0716-1418-1_1). This companion is available at r4ds.io/islr. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["st-edition-vs-2nd-edition.html", "1st edition vs 2nd edition", " 1st edition vs 2nd edition This club is reading the digital version of the second edition of this book (2e). "],["pace.html", "Pace", " Pace This book is often used for two-semester-long courses. We’ll try to cover 1 chapter/week, but… …It’s ok to split chapters when they feel like too much. We will try to meet every week, but will likely take some breaks for holidays, etc. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Learning objectives: Recognize various types of statistical learning. Understand why this book is useful for you. Be able to read mathematical notation used throughout this book. Describe the overall layout of this book. Be able to find data used in examples throughout the book. "],["what-is-statistical-learning.html", "1.1 What is statistical learning?", " 1.1 What is statistical learning? More about this in Chapter 2. Supervised: “Building a model to predict an output from inputs.” Predict wage from age, education, and year. Predict market direction from previous days' performance. Unsupervised: Inputs but no specific outputs, find relationships and structure. Identify clusters within cancer cell lines. "],["why-islr.html", "1.2 Why ISLR?", " 1.2 Why ISLR? “Facilitate the transition of statistical learning from an academic to a mainstream field.” Machine learning* is useful to everyone, let’s all learn enough to use it responsibly. R “labs” make this make sense for this community! "],["notation.html", "1.3 Notation", " 1.3 Notation n = number of observations (rows) p = number of features/variables (columns) We’ll come back here if we need to as we go! Some symbols they assume we know: \\(\\in\\) = “is an element of”, “in” \\({\\rm I\\!R}\\) = “real numbers” "],["what-have-we-gotten-ourselves-into.html", "1.4 What have we gotten ourselves into?", " 1.4 What have we gotten ourselves into? 2: Terminology &amp; main concepts 3-4: Classic linear methods 5: Resampling (so we can choose the best method) 6: Modern updates to linear methods 7+: Beyond Linearity (we can worry about details as we get there) "],["wheres-the-data.html", "1.5 Where’s the data?", " 1.5 Where’s the data? install.packages(&quot;ISLR2&quot;) Or “install” this book. install.packages(&quot;remotes&quot;) remotes::install_github(&quot;r4ds/bookclub-islr&quot;) remove.packages(&quot;bookclubislr&quot;) # This isn&#39;t really a package. "],["meeting-videos.html", "1.6 Meeting Videos", " 1.6 Meeting Videos 1.6.1 Cohort 1 Meeting chat log ADD LOG HERE "],["learning.html", "Chapter 2 Statistical Learning", " Chapter 2 Statistical Learning Learning objectives: Understand Vocabulary for prediction Understand “Error”/Accuracy Understand Parametric vs Nonparametric Models Describe the trade-off between more accurate models and more interpretable models. Compare and contrast supervised and unsupervised learning. Compare and contrast regression and classification problems. Measure the accuracy/goodness of regression model fits. Measure the accuracy/goodness of classification model fits. Describe how bias and variance contribute to the model error. Understand overfitting. Recognize KNN. Understand the role of tuning in ML models. "],["advertising-example.html", "2.1 Advertising Example", " 2.1 Advertising Example Data set consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper. Fig 2.1 "],["terms.html", "2.2 Terms", " 2.2 Terms In this setting, the advertising budgets are input variables while sales is an output variable. The input variables are typically denoted using the symbol \\(X\\), with a subscript to distinguish them. So \\(X_1\\) might be the TV budget, \\(X_2\\) the radio budget, and \\(X_3\\) the newspaper budget. The inputs go by different names, such as predictors, independent variables, features, or sometimes just variables. The output variable in this case, sales is often called the response or dependent variable, and is typically denoted using the symbol \\(Y\\). Throughout this book, we will use all of these terms interchangeably. "],["equation-2.html", "2.3 Equation 2.1", " 2.3 Equation 2.1 We assume that there is some relationship between \\(Y\\) and \\(X = (X_1, X_2,\\dots,X_p)\\), which can be written in the very general form \\[\\begin{equation} y = f(X) + \\epsilon \\end{equation}\\] Here \\(f\\) is some fixed but unknown function of \\(X_1, \\dots, X_p\\), and \\(\\epsilon\\) is a random error term, which is independent of \\(X\\) and has mean zero. In this formulation, \\(f\\) represents the systematic information that \\(X\\) provides about \\(Y\\). "],["error-terms.html", "2.4 Error terms", " 2.4 Error terms Fig 2.2 The vertical lines represent the error terms \\(\\epsilon\\). "],["error-terms-two-predictors.html", "2.5 Error Terms Two Predictors", " 2.5 Error Terms Two Predictors Fig 2.3 Here \\(f\\) is a two-dimensional surface that must be estimated based on the observed data. In essence, statistical learning refers to a set of approaches for estimating \\(f\\). "],["why-estiamte-f.html", "2.6 Why Estiamte \\(f\\)?", " 2.6 Why Estiamte \\(f\\)? There are two main reasons that we may wish to estimate \\(f\\): prediction and inference. "],["prediction.html", "2.7 Prediction", " 2.7 Prediction In many situations, a set of inputs X are readily available, but the output Y cannot be easily obtained. In this setting, since the error term averages to zero, we can predict Y using \\[\\begin{equation} \\hat{Y} = \\hat{f}(X) \\end{equation}\\] \\(\\hat{f}\\) is often treated as a black box, One is not typically concerned with the exact form of \\(\\hat{f}\\), provided that it yields accurate predictions for Y. "],["accuracy.html", "2.8 Accuracy", " 2.8 Accuracy The accuracy of \\(\\hat{y}\\) as a prediction for \\(y\\) depends on two quantities, which we will call the reducible error and the irreducible error. In general, \\(\\hat{f}\\) will not be perfect estimate for \\(f\\), and this inaccuracy will introduce some error. This error is reducible because we can potentially improve the accuracy of \\(\\hat{f}\\) by using the most appropriate statistical learning technique to estimate \\(f\\). Even if it were possible to form a perfect estimate for \\(f\\), our prediction would still have some error in it! This is because \\(Y\\) is also a function of \\(\\epsilon\\), which, by definition, cannot be predicted using \\(X\\). Therefore, variability associated with \\(\\epsilon\\) also affects the accuracy of our predictions. This is known as the irreducible error, because no matter how well we estimate \\(f\\), we cannot reduce the error introduced by \\(\\epsilon\\). The quantity \\(\\epsilon\\) may contain unmeasured variables that are useful in predicting \\(Y\\) "],["reducible-vs-irreducible-error.html", "2.9 Reducible vs Irreducible Error", " 2.9 Reducible vs Irreducible Error Consider a given estimate \\(\\hat{f}\\) and a set of predictors \\(X\\), which yields the prediction \\(\\hat{Y} = \\hat{f}(X)\\). It is easy to show that EQ 2.3 where \\(\\mathrm{E}(Y -\\hat{Y})^2\\) represents the average, or expected value, of the squared difference between the predicted and actual value of \\(Y\\), and Var( \\(\\epsilon\\) ) represents the variance associated with the error term \\(\\epsilon\\). The focus of this book is on techniques for estimating \\(f\\) with the aim of minimizing the reducible error. It is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for \\(Y\\). This bound is almost always unknown in practice. "],["inference.html", "2.10 Inference", " 2.10 Inference We are often interested in understanding the association between \\(Y\\) and \\(X_1,\\dots,X_p\\). In this situation we wish to estimate \\(f\\), but our goal is not necessarily to make predictions for \\(Y\\). Now \\(\\hat{f}\\) cannot be treated as a black box, because we need to know its exact form. Which predictors are associated with the response? What is the relationship between the response and each predictor? Can the relationship between \\(Y\\) and each predictor be adequately summarized using a linear equation, or is the relationship more complicated? "],["linear-model.html", "2.11 Linear Model", " 2.11 Linear Model Depending on whether our ultimate goal is prediction, inference, or a combination of the two, different methods for estimating \\(f\\) may be appropriate. Linear models allow for relatively simple and interpretable inference, but may not yield as accurate predictions as some other approaches. In contrast, some of the highly non-linear approaches that we discuss in the later chapters of this book can potentially provide quite accurate predictions for \\(Y\\), but this comes at the expense of a less interpretable model for which inference is more challenging. "],["parametric-methods-_1.html", "2.12 Parametric methods \\(_{(1)}\\)", " 2.12 Parametric methods \\(_{(1)}\\) We make an assumption about the functional form, or shape, of \\(f\\). For example, one very simple assumption is that \\(f\\) is linear in \\(X\\): \\[ f(X) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ··· + \\beta_pX_p. \\] Instead of having to estimate an entirely arbitrary \\(p\\)-dimensional function \\(f(X)\\), one only needs to estimate the \\(p + 1\\) coefficients \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\). "],["parametric-methods-_2.html", "2.13 Parametric methods \\(_{(2)}\\)", " 2.13 Parametric methods \\(_{(2)}\\) We need a procedure that uses the training data to fit or train the model. In the case of the linear model, we need to estimate the parameters \\(\\beta_0, \\beta_1,\\dots, \\beta_p\\). That is, we want to find values of these parameters such that \\[Y \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_pX_p.\\] The most common approach to fitting the model is referred to as (ordinary) least squares, which we discuss in Chapter 3. "],["parametric-methods-_3---easier-than-an-arbitray-function.html", "2.14 Parametric methods \\(_{(3)}\\) - Easier than an arbitray function", " 2.14 Parametric methods \\(_{(3)}\\) - Easier than an arbitray function The model-based approach just described is referred to as parametric; it reduces the problem of estimating \\(f\\) down to one of estimating a set of parameters. It is generally much easier to estimate a set of parameters, such as \\(\\beta_0, \\beta_1, \\dots, \\beta_y\\) in the linear model, than it is to fit an entirely arbitrary function \\(f\\). "],["parametric-methods-_4---disadvantage.html", "2.15 Parametric methods \\(_{(4)}\\) - Disadvantage", " 2.15 Parametric methods \\(_{(4)}\\) - Disadvantage The potential disadvantage of a parametric approach is that the model we choose will usually not match the true unknown form of \\(f\\). If the chosen model is too far from the true \\(f\\), then our estimate will be poor. We can try to address this problem by choosing flexible models that can fit many different possible functional forms for \\(f\\). These more complex models can lead to a phenomenon known as overfitting the data, which essentially means they follow the errors, or noise, too closely. "],["basic-parametric-model.html", "2.16 Basic Parametric Model", " 2.16 Basic Parametric Model Figure 2-4 Figure shows an example of the parametric approach applied to the Income data. We have fit a linear model of the form \\[\\tt{income} \\approx \\beta_0 + \\beta_1 \\times \\tt{education} + \\beta_2 \\times \\tt{seniority}.\\] We can see that the linear fit given in Figure 2.4 is not quite right: the true \\(f\\) has some curvature that is not captured in the linear fit. "],["non-parametric-methods.html", "2.17 Non-Parametric Methods", " 2.17 Non-Parametric Methods Non-parametric methods don’t make explicit assumptions about the functional form of \\(f\\) Instead they seek an estimate of \\(f\\) that gets as close to the data points as possible without being too rough or wiggly. Any parametric approach brings with it the possibility that the functional form used to estimate \\(f\\) is very different from the true \\(f\\), in which case the resulting model will not fit the data well. In contrast, non-parametric approaches completely avoid this danger, since essentially no assumption about the form of \\(f\\) is made. But non-parametric approaches do suffer from a major disadvantage: very large number of observations (far more a than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for \\(f\\). "],["thin-plate-splines.html", "2.18 Thin plate splines", " 2.18 Thin plate splines A thin-plate spline is used to estimate \\(f\\). This approach does not impose any pre-specified model on \\(f\\). It instead attempts to produce an estimate for \\(f\\) that is as close as possible to the observed data, subject to the fit that is, the yellow surface being smooth. Second plot shows the same thin-plate spline fit using a lower level of smoothness, allowing for a rougher fit. The resulting estimate fits the observed data perfectly! This is an example of overfitting the data. "],["prediction-accuracy-vs-model-interpretability.html", "2.19 Prediction Accuracy vs Model Interpretability", " 2.19 Prediction Accuracy vs Model Interpretability Why would we ever choose to use a more restrictive method instead of a very flexible approach? If we are mainly interested in inference, then restrictive models are much more interpretable. Very flexible approaches, can lead to such complicated estimates of \\(f\\) that it is difficult to understand how any individual predictor is associated with the response. Least squares linear regression is relatively inflexible but is quite interpretable. Lasso, less flexible approach than linear regression. It is also more interpretable Generalized additive models (GAMs), more flexible than linear regression Fully non-linear methods are highly flexible approaches that are harder to interpret. bagging boosting support vector machines with non-linear kernels neural networks (deep learning) "],["supervised-versus-unsupervised-learning.html", "2.20 Supervised Versus Unsupervised Learning", " 2.20 Supervised Versus Unsupervised Learning Most statistical learning problems fall into one of two categories: supervised or unsupervised. With unsupervised learning, there is no response variable to predict. The goal of cluster analysis is to ascertain, on the basis of \\(x_1,\\dots, x_n\\), whether the observations fall into relatively distinct groups. Figure 2-8 "],["regression-versus-classification-problems.html", "2.21 Regression Versus Classification Problems", " 2.21 Regression Versus Classification Problems Variables can be characterized as either quantitative or qualitative (categorical). Quantitative variables take on numerical values. In contrast, qualitative variables take on values in one of K different classes, or categories. "],["assessing-model-accuracy.html", "2.22 Assessing Model Accuracy", " 2.22 Assessing Model Accuracy There is no free lunch in statistics: no one method dominates all others over all possible data sets. On a particular data set, one specific method may work best, but some other method may work better on a similar but different data set. "],["mse.html", "2.23 MSE", " 2.23 MSE We need some way to measure how well its predictions actually match the observed data. In the regression setting, the most commonly-used measure is the mean squared error (MSE), given by \\[ MSE = \\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{f}(x_i))^2,\\] The MSE will be small if the predicted responses are very close to the true responses,and will be large if for some of the observations, the predicted and true responses differ substantially. "],["training-vs.-test.html", "2.24 Training vs. Test", " 2.24 Training vs. Test The MSE in the above equation is computed using the training data that was used to fit the model, and so should more accurately be referred to as the training MSE. In general, we do not really care how well the method works on the training data. We are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data. \\[\\mathrm{Ave}(y_0 - \\hat{f}(x_0))^2 ,\\] We’d like to select the model for which this quantity is as small as possible on future samples. "],["overfitting-with-increased-flexiblity.html", "2.25 Overfitting with Increased Flexiblity", " 2.25 Overfitting with Increased Flexiblity Figure 2-11 "],["interpretation.html", "2.26 Interpretation", " 2.26 Interpretation The degrees of freedom is a quantity that summarizes the flexibility of a curve. The training MSE declines monotonically as flexibility increases. As model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE. "],["the-bias-variance-trade-off.html", "2.27 The Bias-Variance Trade-Off", " 2.27 The Bias-Variance Trade-Off MSE, for a given value, can always be decomposed into the sum of three fundamental quantities: the variance of \\(\\hat{f}(x_0)\\), the squared bias of \\(\\hat{f}(x_0)\\) and the variance of the error terms \\(\\epsilon\\). That is, \\[E\\big(y_0 - \\hat{f}(x_0)\\big)^2 = \\mathrm{Var}\\big(\\hat{f}(x_0)\\big) +[\\mathrm{Bias}\\big(\\hat{f}(x_0)\\big)]^2 + \\mathrm{Var}(\\epsilon)\\] Here the notation \\(E\\big(y_0 - \\hat{f}(x_0)\\big)^2\\) defines the expected test MSE at \\(x_0\\) and refers to the average test MSE that we would obtain if we repeatedly estimated \\(f\\) using a large number of training sets, and tested each at \\(x_0\\). The overall expected test MSE can be computed by averaging \\(E \\big(y_0 - \\hat{f}x(x_0)\\big)^2\\) over all possible values of \\(x_0\\) in the test set We need to select a statistical learning method that simultaneously achieves low variance and low bias. "],["bias-variance-trade-off.html", "2.28 Bias-Variance Trade-Off", " 2.28 Bias-Variance Trade-Off As we use more flexible methods, the variance will increase and the bias will decrease. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases. Figure 2-11 In all three cases, the variance increases and the bias decreases as the method’s flexibility increases. The relationship between bias, variance, and test set MSE given is referred to as the bias-variance trade-off. The challenge lies in finding a method for which both the variance and the squared bias are low. This trade-off is one of the most important recurring themes in this book. "],["the-classification-setting.html", "2.29 The Classification Setting", " 2.29 The Classification Setting The most common approach for quantifying the accuracy of our estimate \\(\\hat{f}\\) is the training error rate, the proportion of mistakes that are made if we apply our estimate \\(\\hat{f}\\) to the training observations: \\[\\frac{1}{n}\\sum_{i=1}^{n}I(y_i \\ne \\hat{y}_i).\\] The above equation computes the fraction of incorrect classifications. Equation is referred to as the training error rate because it is computed based on the data that was used to train our classifier. We are most interested in the error rates that result from applying our classifier to test observations that were not used in training. "],["test-error.html", "2.30 Test Error", " 2.30 Test Error The test error rate associated with a set of test observations of the form \\((x_0, y_0)\\) is given by \\[\\mathrm{Ave}\\big(I(y_i \\ne \\hat{y}_i)\\big).\\] Where \\(\\hat{y}_0\\) is the predicted class label that results from applying the classifier to the test observation with predictor \\(x_0\\). A good classifier is one for which the test error is smallest. "],["the-bayes-classifier.html", "2.31 The Bayes Classifier", " 2.31 The Bayes Classifier The test error rate is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values. In other words, we should simply assign a test observation with predictor vector \\(x_0\\) to the class \\(j\\) for which \\[\\mathrm{Pr}(Y=j|X=x_0).\\] Note that is a conditional probability: it is the probability that \\(Y = j\\), given the observed predictor vector \\(X_0\\). This very simple classifier is called the Bayes classifier. "],["bayes-classifier-decision-boundry.html", "2.32 Bayes Classifier Decision Boundry", " 2.32 Bayes Classifier Decision Boundry Figure 2-13 The purple dashed line represents the points where the probability is exactly 50%. This is called the Bayes decision boundary. An observation that falls on the orange side of the boundary will be assigned to the orange class, and similarly an observation on the blue side of the boundary will be assigned to the blue class. The Bayes classifier produces the lowest possible test error rate, called the Bayes error rate. The overall Bayes error rate is given by \\[1- E\\big(\\mathop{\\mathrm{max}}_{j}(Y=j|X)\\big),\\] where the expectation averages the probability over all possible values of X. The Bayes error rate is analogous to the irreducible error, discussed earlier. "],["k-nearest-neighbors.html", "2.33 K-Nearest Neighbors", " 2.33 K-Nearest Neighbors For real data, we do not know the conditional distribution of \\(Y\\) given \\(X\\), and so computing the Bayes classifier is impossible. Many approaches attempt to estimate the conditional distribution of \\(Y\\) given \\(X\\), and then classify a given observation to the class with highest estimated probability. One such method is the K-nearest neighbors (KNN) classifier. Given a positive integer K and a test observation \\(t_{0}\\), the KNN classifier first identifies the K points in the training data that are closest to \\(x_0\\) \\[\\mathrm{Pr}(Y=j|X=x_0) = \\frac{1}{K}\\sum_{i \\in \\mathcal{N}_0} I (y_i = j)\\] Finally, KNN classifies the test observation \\(x_0\\) to the class with the largest probability. Figure 2-143 The KNN approach with \\(K\\) = 3 at all of the possible values for \\(X_1\\) and \\(X_2\\), and have drawn in the corresponding KNN decision boundary. Despite the fact that it is a very simple approach, KNN can often produce classifiers that are surprisingly close to the optimal Bayes classifier. "],["knn-with-different-k.html", "2.34 KNN with Different K", " 2.34 KNN with Different K The choice of K has a drastic effect on the KNN classifier obtained. "],["knn-tuning.html", "2.35 KNN Tuning", " 2.35 KNN Tuning Figure 2-17 There is not a strong relationship between the training error rate and the test error rate. As we use more flexible classification methods, the training error rate will decline but the test error rate may not. As \\(1/K\\) increases, the method becomes more flexible. As in the regression setting, the training error rate consistently declines as the flexibility increases. However, the test error exhibits a characteristic U-shape, declining at first (with a minimum at approximately \\(K\\) = 10) before increasing again when the method becomes excessively flexible and overfits. "],["linear.html", "Chapter 3 Linear Regression", " Chapter 3 Linear Regression Learning objectives: Perform linear regression with a single predictor variable. Estimate the standard error of regression coefficients. Evaluate the goodness of fit of a regression. Perform linear regression with multiple predictor variables. Evaluate the relative importance of variables in a multiple linear regression. Include interaction effects in a multiple linear regression. Perform linear regression with qualitative predictor variables. Model non-linear relationships using polynomial regression. Identify non-linearity in a data set. Compare and contrast linear regression with KNN regression. "],["questions-to-answer.html", "3.1 Questions to Answer", " 3.1 Questions to Answer Recall the Advertising data from Chapter 2. Here are a few important questions that we might seek to address: Is there a relationship between advertising budget and sales? How strong is the relationship between advertising budget and sales? Does knowledge of the advertising budget provide a lot of information about product sales? Which media are associated with sales? How large is the association between each medium and sales? For every dollar spent on advertising in a particular medium, by what amount will sales increase? How accurately can we predict future sales? Is the relationship linear? If there is approximately a straight-line relationship between advertising expenditure in the various media and sales, then linear regression is an appropriate tool. If not, then it may still be possible to transform the predictor or the response so that linear regression can be used. Is there synergy among the advertising media? Or, in stats terms, is there an interaction effect? "],["simple-linear-regression-definition.html", "3.2 Simple Linear Regression: Definition", " 3.2 Simple Linear Regression: Definition Simple linear regression: Very straightforward approach to predicting response \\(Y\\) on predictor \\(X\\). \\[Y \\approx \\beta_{0} + \\beta_{1}X\\] Read “\\(\\approx\\)” as “is approximately modeled by.” \\(\\beta_{0}\\) = intercept \\(\\beta_{1}\\) = slope \\[\\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x\\] \\(\\hat{\\beta}_{0}\\) = our approximation of intercept \\(\\hat{\\beta}_{1}\\) = our approximation of slope \\(x\\) = sample of \\(X\\) \\(\\hat{y}\\) = our prediction of \\(Y\\) from \\(x\\) "],["simple-linear-regression-visualization.html", "3.3 Simple Linear Regression: Visualization", " 3.3 Simple Linear Regression: Visualization Figure 3.1: For the Advertising data, the least squares fit for the regression of sales onto TV is shown. The fit is found by minimizing the residual sum of squares. Each grey line segment represents a residual. In this case a linear fit captures the essence of the relationship, although it overestimates the trend in the left of the plot. "],["simple-linear-regression-math.html", "3.4 Simple Linear Regression: Math", " 3.4 Simple Linear Regression: Math RSS = residual sum of squares \\[\\mathrm{RSS} = e^{2}_{1} + e^{2}_{2} + \\ldots + e^{2}_{n}\\] \\[\\mathrm{RSS} = (y_{1} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{1})^{2} + (y_{2} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{2})^{2} + \\ldots + (y_{n} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{n})^{2}\\] \\[\\hat{\\beta}_{1} = \\frac{\\sum_{i=1}^{n}{(x_{i}-\\bar{x})(y_{i}-\\bar{y})}}{\\sum_{i=1}^{n}{(x_{i}-\\bar{x})^{2}}}\\] \\[\\hat{\\beta}_{0} = \\bar{y} - \\hat{\\beta}_{1}\\bar{x}\\] \\(\\bar{x}\\), \\(\\bar{y}\\) = sample means of \\(x\\) and \\(y\\) 3.4.1 Visualization of Fit Figure 3.2: Contour and three-dimensional plots of the RSS on the Advertising data, using sales as the response and TV as the predictor. The red dots correspond to the least squares estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\), given by (3.4) Learning Objectives: Perform linear regression with a single predictor variable. ✔️ "],["assessing-accuracy-of-coefficient-estimates.html", "3.5 Assessing Accuracy of Coefficient Estimates", " 3.5 Assessing Accuracy of Coefficient Estimates \\[Y = \\beta_{0} + \\beta_{1}X + \\epsilon\\] RSE = residual standard error Estimate of \\(\\sigma\\) \\[\\mathrm{RSE} = \\sqrt{\\frac{\\mathrm{RSS}}{n - 2}}\\] \\[\\mathrm{SE}(\\hat\\beta_0)^2 = \\sigma^2 \\left[\\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\right],\\ \\ \\mathrm{SE}(\\hat\\beta_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\] 95% confidence interval: a range of values such that with 95% probability, the range will contain the true unknown value of the parameter If we take repeated samples and construct the confidence interval for each sample, 95% of the intervals will contain the true unknown value of the parameter \\[\\hat\\beta_1 \\pm 2\\ \\cdot \\ \\mathrm{SE}(\\hat\\beta_1)\\] \\[\\hat\\beta_0 \\pm 2\\ \\cdot \\ \\mathrm{SE}(\\hat\\beta_0)\\] Learning Objectives: Estimate the standard error of regression coefficients. ✔️ "],["meeting-videos-1.html", "3.6 Meeting Videos", " 3.6 Meeting Videos 3.6.1 Cohort 1 Meeting chat log ADD LOG HERE "],["classification.html", "Chapter 4 Classification", " Chapter 4 Classification Learning objectives: We’ll fill these in. "],["slide-1.html", "4.1 Slide 1", " 4.1 Slide 1 Try to follow a slide-like format. "],["slide-2.html", "4.2 Slide 2", " 4.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-2.html", "4.3 Meeting Videos", " 4.3 Meeting Videos 4.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["resampling-methods.html", "Chapter 5 Resampling Methods", " Chapter 5 Resampling Methods Learning objectives: We’ll fill these in. "],["slide-1-1.html", "5.1 Slide 1", " 5.1 Slide 1 Try to follow a slide-like format. "],["slide-2-1.html", "5.2 Slide 2", " 5.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-3.html", "5.3 Meeting Videos", " 5.3 Meeting Videos 5.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["linear-model-selection-and-regularization.html", "Chapter 6 Linear Model Selection and Regularization", " Chapter 6 Linear Model Selection and Regularization Learning objectives: We’ll fill these in. "],["slide-1-2.html", "6.1 Slide 1", " 6.1 Slide 1 Try to follow a slide-like format. "],["slide-2-2.html", "6.2 Slide 2", " 6.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-4.html", "6.3 Meeting Videos", " 6.3 Meeting Videos 6.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["moving-beyond-linearity.html", "Chapter 7 Moving Beyond Linearity", " Chapter 7 Moving Beyond Linearity Learning objectives: We’ll fill these in. "],["slide-1-3.html", "7.1 Slide 1", " 7.1 Slide 1 Try to follow a slide-like format. "],["slide-2-3.html", "7.2 Slide 2", " 7.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-5.html", "7.3 Meeting Videos", " 7.3 Meeting Videos 7.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["tree-based-methods.html", "Chapter 8 Tree-Based Methods", " Chapter 8 Tree-Based Methods Learning objectives: We’ll fill these in. "],["slide-1-4.html", "8.1 Slide 1", " 8.1 Slide 1 Try to follow a slide-like format. "],["slide-2-4.html", "8.2 Slide 2", " 8.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-6.html", "8.3 Meeting Videos", " 8.3 Meeting Videos 8.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["support-vector-machines.html", "Chapter 9 Support Vector Machines", " Chapter 9 Support Vector Machines Learning objectives: We’ll fill these in. "],["slide-1-5.html", "9.1 Slide 1", " 9.1 Slide 1 Try to follow a slide-like format. "],["slide-2-5.html", "9.2 Slide 2", " 9.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-7.html", "9.3 Meeting Videos", " 9.3 Meeting Videos 9.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["deep-learning.html", "Chapter 10 Deep Learning", " Chapter 10 Deep Learning Learning objectives: We’ll fill these in. "],["slide-1-6.html", "10.1 Slide 1", " 10.1 Slide 1 Try to follow a slide-like format. "],["slide-2-6.html", "10.2 Slide 2", " 10.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-8.html", "10.3 Meeting Videos", " 10.3 Meeting Videos 10.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["survival-analysis-and-censored-data.html", "Chapter 11 Survival Analysis and Censored Data", " Chapter 11 Survival Analysis and Censored Data Learning objectives: We’ll fill these in. "],["slide-1-7.html", "11.1 Slide 1", " 11.1 Slide 1 Try to follow a slide-like format. "],["slide-2-7.html", "11.2 Slide 2", " 11.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-9.html", "11.3 Meeting Videos", " 11.3 Meeting Videos 11.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["unsupervised-learning.html", "Chapter 12 Unsupervised Learning", " Chapter 12 Unsupervised Learning Learning objectives: We’ll fill these in. "],["slide-1-8.html", "12.1 Slide 1", " 12.1 Slide 1 Try to follow a slide-like format. "],["slide-2-8.html", "12.2 Slide 2", " 12.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-10.html", "12.3 Meeting Videos", " 12.3 Meeting Videos 12.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["multiple-testing.html", "Chapter 13 Multiple Testing", " Chapter 13 Multiple Testing Learning objectives: We’ll fill these in. "],["slide-1-9.html", "13.1 Slide 1", " 13.1 Slide 1 Try to follow a slide-like format. "],["slide-2-9.html", "13.2 Slide 2", " 13.2 Slide 2 Use ## to create new slides (sections). "],["meeting-videos-11.html", "13.3 Meeting Videos", " 13.3 Meeting Videos 13.3.1 Cohort 1 Meeting chat log ADD LOG HERE "],["abbreviations.html", "Abbreviations", " Abbreviations Abbreviation Term RSE residual standard error RSS residual sum of squares TSS total sum of squares "],["latex.html", "Appendix: Bookdown and LaTeX Notes", " Appendix: Bookdown and LaTeX Notes Ray’s Formatting Notes: version 2021-09-22 https://gist.github.com/RaymondBalise/ee4b7da0a70087317dc52bf479a4e2b6 .col2 { columns: 2 150px; /* number of columns and width in pixels*/ -webkit-columns: 2 150px; /* chrome, safari */ -moz-columns: 2 150px; /* firefox */ } .col3 { columns: 3 100px; -webkit-columns: 3 100px; -moz-columns: 3 100px; } "],["markdown-highlighting.html", "Markdown highlighting", " Markdown highlighting Formatting Code bold **bold** bold __bold__ italic *italic* italic _italic_ "],["text-coloring.html", "Text coloring", " Text coloring To add color you can use CSS or R code like this: color_text &lt;- function(x, color){ if(knitr::is_latex_output()) paste(&quot;\\\\textcolor{&quot;,color,&quot;}{&quot;,x,&quot;}&quot;,sep=&quot;&quot;) else if(knitr::is_html_output()) paste(&quot;&lt;font color=&#39;&quot;,color,&quot;&#39;&gt;&quot;,x,&quot;&lt;/font&gt;&quot;,sep=&quot;&quot;) else x } red &lt;- function(x){ if(knitr::is_latex_output()) paste(&quot;\\\\textcolor{&quot;,&#39;red&#39;,&quot;}{&quot;,x,&quot;}&quot;,sep=&quot;&quot;) else if(knitr::is_html_output()) paste(&quot;&lt;font color=&#39;red&#39;&gt;&quot;,x,&quot;&lt;/font&gt;&quot;,sep=&quot;&quot;) else x } The strng This is colored with the red function comes from: `r red(&quot;This is colored with the red function&quot;)` The string This is colored with the color_text function set to mauve comes from: `r color_text(&quot;This is colored with the color_text function set to mauve&quot;, &quot;#E0B0FF&quot;)` "],["x99-4.html", "Section references", " Section references Section 99.4 comes from Section [99.4](#x99-4) "],["footnotes.html", "Footnotes", " Footnotes 1 comes from ^[A footnote] A footnote↩︎ "],["formatting-text.html", "Formatting Text", " Formatting Text Appearance Code Three xfun::n2w(3, cap = TRUE) 14.5% scales::percent(0.1447, accuracy= .1) p=0.145 scales::pvalue(0.1447, accuracy= .001, add_p =TRUE) "],["figures.html", "Figures", " Figures Name a figure chunk with a name like figure99-1 and include fig.cap=“something” then reference it like this: \\@ref(fig:figure99-1) "],["displaying-formula.html", "Displaying Formula", " Displaying Formula Formatting To tweak the appearance of words use these formats: Formatting Code Looks like plain text \\text{text Pr} \\(\\text{text Pr}\\) bold Greek symbol \\boldsymbol{\\epsilon} \\(\\boldsymbol{\\epsilon}\\) typewriter \\tt{blah} \\(\\tt{blah}\\) slide font \\sf{blah} \\(\\sf{blah}\\) bold \\mathbf{x} \\(\\mathbf{x}\\) plain \\mathrm{text Pr} \\(\\mathrm{text Pr}\\) cursive \\mathcal{S} \\(\\mathcal{S}\\) Blackboard bold \\mathbb{R} \\(\\mathbb{R}\\) Symbols Symbols Code \\(\\stackrel{\\text{def}}{=}\\) \\stackrel{\\text{def}}{=} Notation Based on: https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html Math Code \\(x = y\\) $x = y$ \\(x \\approx y\\) $x \\approx y$ \\(x &lt; y\\) $x &lt; y$ \\(x &gt; y\\) $x &gt; y$ \\(x \\le y\\) $x \\le y$ \\(x \\ge y\\) $x \\ge y$ \\(x \\ge y\\) $x \\ge y$ \\(x \\times y\\) $x \\times y$ \\(x^{n}\\) $x^{n}$ \\(x_{n}\\) $x_{n}$ \\(\\overline{x}\\) $\\overline{x}$ \\(\\hat{x}\\) $\\hat{x}$ \\(\\widehat{SE}\\) $\\widehat{SE}$ \\(\\tilde{x}\\) $\\tilde{x}$ \\(\\frac{a}{b}\\) $\\frac{a}{b}$ \\(\\displaystyle \\frac{a}{b}\\) $\\displaystyle \\frac{a}{b}$ \\(\\binom{n}{k}\\) $\\binom{n}{k}$ \\(x_{1} + x_{2} + \\cdots + x_{n}\\) $x_{1} + x_{2} + \\cdots + x_{n}$ \\(x_{1}, x_{2}, \\dots, x_{n}\\) $x_{1}, x_{2}, \\dots, x_{n}$ \\(\\mathbf{x} = \\langle x_{1}, x_{2}, \\dots, x_{n}\\rangle\\) $\\mathbf{x} = \\langle x_{1}, x_{2}, \\dots, x_{n}\\rangle$ \\(x \\in A\\) $x \\in A$ \\(|A|\\) $|A|$ \\(x \\in A\\) $x \\in A$ \\(x \\subset B\\) $x \\subset B$ \\(x \\subseteq B\\) $x \\subseteq B$ \\(A \\cup B\\) $A \\cup B$ \\(A \\cap B\\) $A \\cap B$ \\(X \\sim {\\sf Binom}(n, \\pi)\\) X \\sim {\\sf Binom}(n, \\pi)$ \\(\\mathrm{P}(X \\le x) = {\\tt pbinom}(x, n, \\pi)\\) $\\mathrm{P}(X \\le x) = {\\tt pbinom}(x, n, \\pi)$ \\(P(A \\mid B)\\) $P(A \\mid B)$ \\(\\mathrm{P}(A \\mid B)\\) $\\mathrm{P}(A \\mid B)$ \\(\\{1, 2, 3\\}\\) $\\{1, 2, 3\\}$ \\(\\sin(x)\\) $\\sin(x)$ \\(\\log(x)\\) $\\log(x)$ \\(\\int_{a}^{b}\\) $\\int_{a}^{b}$ \\(\\left(\\int_{a}^{b} f(x) \\; dx\\right)\\) $\\left(\\int_{a}^{b} f(x) \\; dx\\right)$ \\(\\left[\\int_{-\\infty}^{\\infty} f(x) \\; dx\\right]\\) $\\left[\\int_{\\-infty}^{\\infty} f(x) \\; dx\\right]$ \\(\\left. F(x) \\right|_{a}^{b}\\) $\\left. F(x) \\right|_{a}^{b}$ \\(\\sum_{x = a}^{b} f(x)\\) $\\sum_{x = a}^{b} f(x)$ \\(\\prod_{x = a}^{b} f(x)\\) $\\prod_{x = a}^{b} f(x)$ \\(\\lim_{x \\to \\infty} f(x)\\) $\\lim_{x \\to \\infty} f(x)$ \\(\\displaystyle \\lim_{x \\to \\infty} f(x)\\) $\\displaystyle \\lim_{x \\to \\infty} f(x)$ ` \\(RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (Y_n - \\hat{Y}_i)^2}\\) $RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (Y_n - \\hat{Y}_i)^2}$ ` "],["equations.html", "Equations", " Equations These are formulas that appear with an equation number. Basic Equation The names of equations can not include . or _ but it can include - \\begin{equation} 1 + 1 = 2 (\\#eq:eq99-1) \\end{equation} Which appears as: \\[\\begin{equation} 1 + 1 = 2 \\tag{13.1} \\end{equation}\\] The reference to the equation is (13.1) which comes from this code \\@ref(eq:eq99-1) Case-When Equation (Large Curly Brace) Based on: https://tex.stackexchange.com/questions/9065/large-braces-for-specifying-values-of-variables-by-condition Case when formula: \\[\\begin{equation} y = \\begin{cases} 0, &amp; \\text{if}\\ a=1 \\\\ 1, &amp; \\text{otherwise} \\end{cases} \\tag{13.2} \\end{equation} \\] Which comes from this code: \\begin{equation} y = \\begin{cases} 0, &amp; \\text{if}\\ a=1 \\\\ 1, &amp; \\text{otherwise} \\end{cases} (\\#eq:eq99-2) \\begin{equation} The reference to equation is (13.2) which comes from this code \\@ref(eq:eq99-2) Alligned with Underbars \\[\\begin{equation} \\begin{aligned} \\mathrm{E}(Y-\\hat{Y})^2 &amp; = \\mathrm{E}[f(X) + \\epsilon -\\hat{f}(X)]^2 \\\\ &amp; = \\underbrace{[f(X) -\\hat{f}(X)]^2}_{\\mathrm{Reducible}} + \\underbrace{\\mathrm{var}(\\epsilon)}_{\\mathrm{Irreducible}} \\\\ \\end{aligned} \\tag{13.3} \\end{equation}\\] Comes from this code: \\begin{equation} \\begin{aligned} \\mathrm{E}(Y-\\hat{Y})^2 &amp; = \\mathrm{E}[f(X) + \\epsilon -\\hat{f}(X)]^2 \\\\ &amp; = \\underbrace{[f(X) -\\hat{f}(X)]^2}_{\\mathrm{Reducible}} + \\underbrace{\\mathrm{var}(\\epsilon)}_{\\mathrm{Irreducible}} \\\\ \\end{aligned} (\\#eq:eq99-3) \\end{equation} "],["greek-letters.html", "Greek letters", " Greek letters Based on: https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html letters code \\(\\alpha A\\) $\\alpha A$ \\(\\beta B\\) $\\beta B$ \\(\\gamma \\Gamma\\) $\\gamma \\Gamma$ \\(\\delta \\Delta\\) $\\delta \\Delta$ \\(\\epsilon \\varepsilon E\\) $\\epsilon \\varepsilon E$ \\(\\zeta Z \\sigma\\) $\\zeta Z \\sigma \\(\\eta H\\) $\\eta H$ \\(\\theta \\vartheta \\Theta\\) $\\theta \\vartheta \\Theta$ \\(\\iota I\\) $\\iota I$ \\(\\kappa K\\) $\\kappa K$ \\(\\lambda \\Lambda\\) $\\lambda \\Lambda$ \\(\\mu M\\) $\\mu M$ \\(\\nu N\\) $\\nu N$ \\(\\xi\\Xi\\) $\\xi\\Xi$ \\(o O\\) $o O$ (omicron) \\(\\pi \\Pi\\) $\\pi \\Pi$ \\(\\rho\\varrho P\\) $\\rho\\varrho P$ \\(\\sigma \\Sigma\\) \\sigma \\Sigma$ \\(\\tau T\\) $\\tau T$ \\(\\upsilon \\Upsilon\\) $\\upsilon \\Upsilon$ \\(\\phi \\varphi \\Phi\\) $\\phi \\varphi \\Phi$ \\(\\chi X\\) $\\chi X$ \\(\\psi \\Psi\\) $\\psi \\Psi$ \\(\\omega \\Omega\\) $\\omega \\Omega$ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
