<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.5 A Comparison of Classification Methods | Introduction to Statistical Learning Using R Book Club</title>
  <meta name="description" content="This is the product of the R4DS Online Learning Community’s Introduction to Statistical Learning Using R Book Club." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="4.5 A Comparison of Classification Methods | Introduction to Statistical Learning Using R Book Club" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the product of the R4DS Online Learning Community’s Introduction to Statistical Learning Using R Book Club." />
  <meta name="github-repo" content="r4ds/bookclub-islr" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.5 A Comparison of Classification Methods | Introduction to Statistical Learning Using R Book Club" />
  
  <meta name="twitter:description" content="This is the product of the R4DS Online Learning Community’s Introduction to Statistical Learning Using R Book Club." />
  

<meta name="author" content="The R4DS Online Learning Community" />


<meta name="date" content="2021-11-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="generative-models-for-classification.html"/>
<link rel="next" href="summary-of-the-classification-methods.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Introduction to Statistical Learning Using R Book Club</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a><ul>
<li class="chapter" data-level="" data-path="book-club-meetings.html"><a href="book-club-meetings.html"><i class="fa fa-check"></i>Book club meetings</a></li>
<li class="chapter" data-level="" data-path="st-edition-vs-2nd-edition.html"><a href="st-edition-vs-2nd-edition.html"><i class="fa fa-check"></i>1st edition vs 2nd edition</a></li>
<li class="chapter" data-level="" data-path="pace.html"><a href="pace.html"><i class="fa fa-check"></i>Pace</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="what-is-statistical-learning.html"><a href="what-is-statistical-learning.html"><i class="fa fa-check"></i><b>1.1</b> What is statistical learning?</a></li>
<li class="chapter" data-level="1.2" data-path="why-islr.html"><a href="why-islr.html"><i class="fa fa-check"></i><b>1.2</b> Why ISLR?</a></li>
<li class="chapter" data-level="1.3" data-path="notation.html"><a href="notation.html"><i class="fa fa-check"></i><b>1.3</b> Notation</a></li>
<li class="chapter" data-level="1.4" data-path="what-have-we-gotten-ourselves-into.html"><a href="what-have-we-gotten-ourselves-into.html"><i class="fa fa-check"></i><b>1.4</b> What have we gotten ourselves into?</a></li>
<li class="chapter" data-level="1.5" data-path="wheres-the-data.html"><a href="wheres-the-data.html"><i class="fa fa-check"></i><b>1.5</b> Where’s the data?</a></li>
<li class="chapter" data-level="1.6" data-path="meeting-videos.html"><a href="meeting-videos.html"><i class="fa fa-check"></i><b>1.6</b> Meeting Videos</a><ul>
<li class="chapter" data-level="1.6.1" data-path="meeting-videos.html"><a href="meeting-videos.html#cohort-1"><i class="fa fa-check"></i><b>1.6.1</b> Cohort 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="learning.html"><a href="learning.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a><ul>
<li class="chapter" data-level="2.1" data-path="advertising-example.html"><a href="advertising-example.html"><i class="fa fa-check"></i><b>2.1</b> Advertising Example</a></li>
<li class="chapter" data-level="2.2" data-path="terms.html"><a href="terms.html"><i class="fa fa-check"></i><b>2.2</b> Terms</a></li>
<li class="chapter" data-level="2.3" data-path="equation-2.html"><a href="equation-2.html"><i class="fa fa-check"></i><b>2.3</b> Equation 2.1</a></li>
<li class="chapter" data-level="2.4" data-path="error-terms.html"><a href="error-terms.html"><i class="fa fa-check"></i><b>2.4</b> Error terms</a></li>
<li class="chapter" data-level="2.5" data-path="error-terms-two-predictors.html"><a href="error-terms-two-predictors.html"><i class="fa fa-check"></i><b>2.5</b> Error Terms Two Predictors</a></li>
<li class="chapter" data-level="2.6" data-path="why-estiamte-f.html"><a href="why-estiamte-f.html"><i class="fa fa-check"></i><b>2.6</b> Why Estiamte <span class="math inline">\(f\)</span>?</a></li>
<li class="chapter" data-level="2.7" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>2.7</b> Prediction</a></li>
<li class="chapter" data-level="2.8" data-path="accuracy.html"><a href="accuracy.html"><i class="fa fa-check"></i><b>2.8</b> Accuracy</a></li>
<li class="chapter" data-level="2.9" data-path="reducible-vs-irreducible-error.html"><a href="reducible-vs-irreducible-error.html"><i class="fa fa-check"></i><b>2.9</b> Reducible vs Irreducible Error</a></li>
<li class="chapter" data-level="2.10" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>2.10</b> Inference</a></li>
<li class="chapter" data-level="2.11" data-path="linear-model.html"><a href="linear-model.html"><i class="fa fa-check"></i><b>2.11</b> Linear Model</a></li>
<li class="chapter" data-level="2.12" data-path="parametric-methods-_1.html"><a href="parametric-methods-_1.html"><i class="fa fa-check"></i><b>2.12</b> Parametric methods <span class="math inline">\(_{(1)}\)</span></a></li>
<li class="chapter" data-level="2.13" data-path="parametric-methods-_2.html"><a href="parametric-methods-_2.html"><i class="fa fa-check"></i><b>2.13</b> Parametric methods <span class="math inline">\(_{(2)}\)</span></a></li>
<li class="chapter" data-level="2.14" data-path="parametric-methods-_3---easier-than-an-arbitray-function.html"><a href="parametric-methods-_3---easier-than-an-arbitray-function.html"><i class="fa fa-check"></i><b>2.14</b> Parametric methods <span class="math inline">\(_{(3)}\)</span> - Easier than an arbitray function</a></li>
<li class="chapter" data-level="2.15" data-path="parametric-methods-_4---disadvantage.html"><a href="parametric-methods-_4---disadvantage.html"><i class="fa fa-check"></i><b>2.15</b> Parametric methods <span class="math inline">\(_{(4)}\)</span> - Disadvantage</a></li>
<li class="chapter" data-level="2.16" data-path="basic-parametric-model.html"><a href="basic-parametric-model.html"><i class="fa fa-check"></i><b>2.16</b> Basic Parametric Model</a></li>
<li class="chapter" data-level="2.17" data-path="non-parametric-methods.html"><a href="non-parametric-methods.html"><i class="fa fa-check"></i><b>2.17</b> Non-Parametric Methods</a></li>
<li class="chapter" data-level="2.18" data-path="thin-plate-splines.html"><a href="thin-plate-splines.html"><i class="fa fa-check"></i><b>2.18</b> Thin plate splines</a></li>
<li class="chapter" data-level="2.19" data-path="prediction-accuracy-vs-model-interpretability.html"><a href="prediction-accuracy-vs-model-interpretability.html"><i class="fa fa-check"></i><b>2.19</b> Prediction Accuracy vs Model Interpretability</a></li>
<li class="chapter" data-level="2.20" data-path="supervised-versus-unsupervised-learning.html"><a href="supervised-versus-unsupervised-learning.html"><i class="fa fa-check"></i><b>2.20</b> Supervised Versus Unsupervised Learning</a></li>
<li class="chapter" data-level="2.21" data-path="regression-versus-classification-problems.html"><a href="regression-versus-classification-problems.html"><i class="fa fa-check"></i><b>2.21</b> Regression Versus Classification Problems</a></li>
<li class="chapter" data-level="2.22" data-path="assessing-model-accuracy.html"><a href="assessing-model-accuracy.html"><i class="fa fa-check"></i><b>2.22</b> Assessing Model Accuracy</a></li>
<li class="chapter" data-level="2.23" data-path="mse.html"><a href="mse.html"><i class="fa fa-check"></i><b>2.23</b> MSE</a></li>
<li class="chapter" data-level="2.24" data-path="training-vs.-test.html"><a href="training-vs.-test.html"><i class="fa fa-check"></i><b>2.24</b> Training vs. Test</a></li>
<li class="chapter" data-level="2.25" data-path="overfitting-with-increased-flexiblity.html"><a href="overfitting-with-increased-flexiblity.html"><i class="fa fa-check"></i><b>2.25</b> Overfitting with Increased Flexiblity</a></li>
<li class="chapter" data-level="2.26" data-path="interpretation.html"><a href="interpretation.html"><i class="fa fa-check"></i><b>2.26</b> Interpretation</a></li>
<li class="chapter" data-level="2.27" data-path="the-bias-variance-trade-off.html"><a href="the-bias-variance-trade-off.html"><i class="fa fa-check"></i><b>2.27</b> The Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="2.28" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>2.28</b> Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="2.29" data-path="the-classification-setting.html"><a href="the-classification-setting.html"><i class="fa fa-check"></i><b>2.29</b> The Classification Setting</a></li>
<li class="chapter" data-level="2.30" data-path="test-error.html"><a href="test-error.html"><i class="fa fa-check"></i><b>2.30</b> Test Error</a></li>
<li class="chapter" data-level="2.31" data-path="the-bayes-classifier.html"><a href="the-bayes-classifier.html"><i class="fa fa-check"></i><b>2.31</b> The Bayes Classifier</a></li>
<li class="chapter" data-level="2.32" data-path="bayes-classifier-decision-boundry.html"><a href="bayes-classifier-decision-boundry.html"><i class="fa fa-check"></i><b>2.32</b> Bayes Classifier Decision Boundry</a></li>
<li class="chapter" data-level="2.33" data-path="k-nearest-neighbors.html"><a href="k-nearest-neighbors.html"><i class="fa fa-check"></i><b>2.33</b> K-Nearest Neighbors</a></li>
<li class="chapter" data-level="2.34" data-path="knn-with-different-k.html"><a href="knn-with-different-k.html"><i class="fa fa-check"></i><b>2.34</b> KNN with Different K</a></li>
<li class="chapter" data-level="2.35" data-path="knn-tuning.html"><a href="knn-tuning.html"><i class="fa fa-check"></i><b>2.35</b> KNN Tuning</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear.html"><a href="linear.html"><i class="fa fa-check"></i><b>3</b> Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="questions-to-answer.html"><a href="questions-to-answer.html"><i class="fa fa-check"></i><b>3.1</b> Questions to Answer</a></li>
<li class="chapter" data-level="3.2" data-path="simple-linear-regression-definition.html"><a href="simple-linear-regression-definition.html"><i class="fa fa-check"></i><b>3.2</b> Simple Linear Regression: Definition</a></li>
<li class="chapter" data-level="3.3" data-path="simple-linear-regression-visualization.html"><a href="simple-linear-regression-visualization.html"><i class="fa fa-check"></i><b>3.3</b> Simple Linear Regression: Visualization</a></li>
<li class="chapter" data-level="3.4" data-path="simple-linear-regression-math.html"><a href="simple-linear-regression-math.html"><i class="fa fa-check"></i><b>3.4</b> Simple Linear Regression: Math</a><ul>
<li class="chapter" data-level="3.4.1" data-path="simple-linear-regression-math.html"><a href="simple-linear-regression-math.html#visualization-of-fit"><i class="fa fa-check"></i><b>3.4.1</b> Visualization of Fit</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="assessing-accuracy-of-coefficient-estimates.html"><a href="assessing-accuracy-of-coefficient-estimates.html"><i class="fa fa-check"></i><b>3.5</b> Assessing Accuracy of Coefficient Estimates</a></li>
<li class="chapter" data-level="3.6" data-path="meeting-videos-1.html"><a href="meeting-videos-1.html"><i class="fa fa-check"></i><b>3.6</b> Meeting Videos</a><ul>
<li class="chapter" data-level="3.6.1" data-path="meeting-videos-1.html"><a href="meeting-videos-1.html#cohort-1-1"><i class="fa fa-check"></i><b>3.6.1</b> Cohort 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>4</b> Classification</a><ul>
<li class="chapter" data-level="4.1" data-path="an-overview-of-classification.html"><a href="an-overview-of-classification.html"><i class="fa fa-check"></i><b>4.1</b> An Overview of Classification</a></li>
<li class="chapter" data-level="4.2" data-path="why-not-linear-regression.html"><a href="why-not-linear-regression.html"><i class="fa fa-check"></i><b>4.2</b> Why NOT Linear Regression?</a></li>
<li class="chapter" data-level="4.3" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>4.3</b> Logistic Regression</a><ul>
<li class="chapter" data-level="4.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>4.3.1</b> Multiple Logistic Regression</a></li>
<li class="chapter" data-level="4.3.2" data-path="logistic-regression.html"><a href="logistic-regression.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>4.3.2</b> Multinomial Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="generative-models-for-classification.html"><a href="generative-models-for-classification.html"><i class="fa fa-check"></i><b>4.4</b> Generative Models for Classification</a></li>
<li class="chapter" data-level="4.5" data-path="a-comparison-of-classification-methods.html"><a href="a-comparison-of-classification-methods.html"><i class="fa fa-check"></i><b>4.5</b> A Comparison of Classification Methods</a><ul>
<li class="chapter" data-level="4.5.1" data-path="a-comparison-of-classification-methods.html"><a href="a-comparison-of-classification-methods.html#linear-discriminant-analysis-for-p-1"><i class="fa fa-check"></i><b>4.5.1</b> Linear Discriminant Analysis for p = 1</a></li>
<li class="chapter" data-level="4.5.2" data-path="a-comparison-of-classification-methods.html"><a href="a-comparison-of-classification-methods.html#linear-discriminant-analysis-for-p-1-1"><i class="fa fa-check"></i><b>4.5.2</b> Linear Discriminant Analysis for p &gt; 1</a></li>
<li class="chapter" data-level="4.5.3" data-path="a-comparison-of-classification-methods.html"><a href="a-comparison-of-classification-methods.html#quadratic-discriminant-analysis-qda"><i class="fa fa-check"></i><b>4.5.3</b> Quadratic Discriminant Analysis (QDA)</a></li>
<li class="chapter" data-level="4.5.4" data-path="a-comparison-of-classification-methods.html"><a href="a-comparison-of-classification-methods.html#naive-bayes"><i class="fa fa-check"></i><b>4.5.4</b> Naive Bayes</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="summary-of-the-classification-methods.html"><a href="summary-of-the-classification-methods.html"><i class="fa fa-check"></i><b>4.6</b> Summary of the classification methods</a><ul>
<li class="chapter" data-level="4.6.1" data-path="summary-of-the-classification-methods.html"><a href="summary-of-the-classification-methods.html#an-analytical-comparison"><i class="fa fa-check"></i><b>4.6.1</b> An Analytical Comparison</a></li>
<li class="chapter" data-level="4.6.2" data-path="summary-of-the-classification-methods.html"><a href="summary-of-the-classification-methods.html#an-empirical-comparison"><i class="fa fa-check"></i><b>4.6.2</b> An Empirical Comparison</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>4.7</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="4.8" data-path="linear-regression-with-count-data---negative-values.html"><a href="linear-regression-with-count-data---negative-values.html"><i class="fa fa-check"></i><b>4.8</b> Linear regression with count data - negative values</a></li>
<li class="chapter" data-level="4.9" data-path="linear-regression-with-count-data---heteroscedasticity.html"><a href="linear-regression-with-count-data---heteroscedasticity.html"><i class="fa fa-check"></i><b>4.9</b> Linear regression with count data - heteroscedasticity</a></li>
<li class="chapter" data-level="4.10" data-path="problems-with-linear-regression-of-count-data.html"><a href="problems-with-linear-regression-of-count-data.html"><i class="fa fa-check"></i><b>4.10</b> Problems with linear regression of count data</a></li>
<li class="chapter" data-level="4.11" data-path="poisson-distribution.html"><a href="poisson-distribution.html"><i class="fa fa-check"></i><b>4.11</b> Poisson distribution</a></li>
<li class="chapter" data-level="4.12" data-path="poisson-regression-model-mean-lambda.html"><a href="poisson-regression-model-mean-lambda.html"><i class="fa fa-check"></i><b>4.12</b> Poisson Regression Model mean (lambda)</a></li>
<li class="chapter" data-level="4.13" data-path="estimating-the-poisson-regression-parameters.html"><a href="estimating-the-poisson-regression-parameters.html"><i class="fa fa-check"></i><b>4.13</b> Estimating the Poisson Regression parameters</a></li>
<li class="chapter" data-level="4.14" data-path="interpreting-poisson-regression.html"><a href="interpreting-poisson-regression.html"><i class="fa fa-check"></i><b>4.14</b> Interpreting Poisson Regression</a></li>
<li class="chapter" data-level="4.15" data-path="advantages-of-poisson-regression.html"><a href="advantages-of-poisson-regression.html"><i class="fa fa-check"></i><b>4.15</b> Advantages of Poisson Regression</a></li>
<li class="chapter" data-level="4.16" data-path="generalized-linear-models-1.html"><a href="generalized-linear-models-1.html"><i class="fa fa-check"></i><b>4.16</b> Generalized Linear Models</a></li>
<li class="chapter" data-level="4.17" data-path="lab-classification-methods.html"><a href="lab-classification-methods.html"><i class="fa fa-check"></i><b>4.17</b> Lab: Classification Methods</a></li>
<li class="chapter" data-level="4.18" data-path="meeting-videos-2.html"><a href="meeting-videos-2.html"><i class="fa fa-check"></i><b>4.18</b> Meeting Videos</a><ul>
<li class="chapter" data-level="4.18.1" data-path="meeting-videos-2.html"><a href="meeting-videos-2.html#cohort-1-2"><i class="fa fa-check"></i><b>4.18.1</b> Cohort 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="resampling-methods.html"><a href="resampling-methods.html"><i class="fa fa-check"></i><b>5</b> Resampling Methods</a><ul>
<li class="chapter" data-level="5.1" data-path="validation-set-approach.html"><a href="validation-set-approach.html"><i class="fa fa-check"></i><b>5.1</b> Validation Set Approach</a></li>
<li class="chapter" data-level="5.2" data-path="validation-error-rate-varies-depending-on-data-set.html"><a href="validation-error-rate-varies-depending-on-data-set.html"><i class="fa fa-check"></i><b>5.2</b> Validation Error Rate Varies Depending on Data Set</a></li>
<li class="chapter" data-level="5.3" data-path="leave-one-out-cross-validation-loocv.html"><a href="leave-one-out-cross-validation-loocv.html"><i class="fa fa-check"></i><b>5.3</b> Leave-One-Out Cross-Validation (LOOCV)</a></li>
<li class="chapter" data-level="5.4" data-path="advantages-of-loocv-over-validation-set-approach.html"><a href="advantages-of-loocv-over-validation-set-approach.html"><i class="fa fa-check"></i><b>5.4</b> Advantages of LOOCV over Validation Set Approach</a></li>
<li class="chapter" data-level="5.5" data-path="k-fold-cross-validation.html"><a href="k-fold-cross-validation.html"><i class="fa fa-check"></i><b>5.5</b> k-fold Cross-Validation</a></li>
<li class="chapter" data-level="5.6" data-path="graphical-illustration-of-k-fold-approach.html"><a href="graphical-illustration-of-k-fold-approach.html"><i class="fa fa-check"></i><b>5.6</b> Graphical Illustration of k-fold Approach</a></li>
<li class="chapter" data-level="5.7" data-path="advantages-of-k-fold-cross-validation-over-loocv.html"><a href="advantages-of-k-fold-cross-validation-over-loocv.html"><i class="fa fa-check"></i><b>5.7</b> Advantages of k-fold Cross-Validation over LOOCV</a></li>
<li class="chapter" data-level="5.8" data-path="bias-variance-tradeoff-and-k-fold-cross-validation.html"><a href="bias-variance-tradeoff-and-k-fold-cross-validation.html"><i class="fa fa-check"></i><b>5.8</b> Bias-Variance Tradeoff and k-fold Cross-Validation</a></li>
<li class="chapter" data-level="5.9" data-path="cross-validation-on-classification-problems.html"><a href="cross-validation-on-classification-problems.html"><i class="fa fa-check"></i><b>5.9</b> Cross-Validation on Classification Problems</a></li>
<li class="chapter" data-level="5.10" data-path="logistic-polynomial-regression-bayes-decision-boundaries-and-k-fold-cross-validation.html"><a href="logistic-polynomial-regression-bayes-decision-boundaries-and-k-fold-cross-validation.html"><i class="fa fa-check"></i><b>5.10</b> Logistic Polynomial Regression, Bayes Decision Boundaries, and k-fold Cross Validation</a></li>
<li class="chapter" data-level="5.11" data-path="the-bootstrap.html"><a href="the-bootstrap.html"><i class="fa fa-check"></i><b>5.11</b> The Bootstrap</a></li>
<li class="chapter" data-level="5.12" data-path="population-distribution-compared-to-bootstrap-distribution.html"><a href="population-distribution-compared-to-bootstrap-distribution.html"><i class="fa fa-check"></i><b>5.12</b> Population Distribution Compared to Bootstrap Distribution</a></li>
<li class="chapter" data-level="5.13" data-path="bootstrap-standard-error.html"><a href="bootstrap-standard-error.html"><i class="fa fa-check"></i><b>5.13</b> Bootstrap Standard Error</a></li>
<li class="chapter" data-level="5.14" data-path="meeting-videos-3.html"><a href="meeting-videos-3.html"><i class="fa fa-check"></i><b>5.14</b> Meeting Videos</a><ul>
<li class="chapter" data-level="5.14.1" data-path="meeting-videos-3.html"><a href="meeting-videos-3.html#cohort-1-3"><i class="fa fa-check"></i><b>5.14.1</b> Cohort 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html"><i class="fa fa-check"></i><b>6</b> Linear Model Selection and Regularization</a><ul>
<li class="chapter" data-level="6.1" data-path="slide-1.html"><a href="slide-1.html"><i class="fa fa-check"></i><b>6.1</b> Slide 1</a></li>
<li class="chapter" data-level="6.2" data-path="slide-2.html"><a href="slide-2.html"><i class="fa fa-check"></i><b>6.2</b> Slide 2</a></li>
<li class="chapter" data-level="6.3" data-path="meeting-videos-4.html"><a href="meeting-videos-4.html"><i class="fa fa-check"></i><b>6.3</b> Meeting Videos</a><ul>
<li class="chapter" data-level="6.3.1" data-path="meeting-videos-4.html"><a href="meeting-videos-4.html#cohort-1-4"><i class="fa fa-check"></i><b>6.3.1</b> Cohort 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html"><i class="fa fa-check"></i><b>7</b> Moving Beyond Linearity</a><ul>
<li class="chapter" data-level="7.1" data-path="slide-1-1.html"><a href="slide-1-1.html"><i class="fa fa-check"></i><b>7.1</b> Slide 1</a></li>
<li class="chapter" data-level="7.2" data-path="slide-2-1.html"><a href="slide-2-1.html"><i class="fa fa-check"></i><b>7.2</b> Slide 2</a></li>
<li class="chapter" data-level="7.3" data-path="meeting-videos-5.html"><a href="meeting-videos-5.html"><i class="fa fa-check"></i><b>7.3</b> Meeting Videos</a><ul>
<li class="chapter" data-level="7.3.1" data-path="meeting-videos-5.html"><a href="meeting-videos-5.html#cohort-1-5"><i class="fa fa-check"></i><b>7.3.1</b> Cohort 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>8</b> Tree-Based Methods</a><ul>
<li class="chapter" data-level="8.1" data-path="slide-1-2.html"><a href="slide-1-2.html"><i class="fa fa-check"></i><b>8.1</b> Slide 1</a></li>
<li class="chapter" data-level="8.2" data-path="slide-2-2.html"><a href="slide-2-2.html"><i class="fa fa-check"></i><b>8.2</b> Slide 2</a></li>
<li class="chapter" data-level="8.3" data-path="meeting-videos-6.html"><a href="meeting-videos-6.html"><i class="fa fa-check"></i><b>8.3</b> Meeting Videos</a><ul>
<li class="chapter" data-level="8.3.1" data-path="meeting-videos-6.html"><a href="meeting-videos-6.html#cohort-1-6"><i class="fa fa-check"></i><b>8.3.1</b> Cohort 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>9</b> Support Vector Machines</a><ul>
<li class="chapter" data-level="9.1" data-path="slide-1-3.html"><a href="slide-1-3.html"><i class="fa fa-check"></i><b>9.1</b> Slide 1</a></li>
<li class="chapter" data-level="9.2" data-path="slide-2-3.html"><a href="slide-2-3.html"><i class="fa fa-check"></i><b>9.2</b> Slide 2</a></li>
<li class="chapter" data-level="9.3" data-path="meeting-videos-7.html"><a href="meeting-videos-7.html"><i class="fa fa-check"></i><b>9.3</b> Meeting Videos</a><ul>
<li class="chapter" data-level="9.3.1" data-path="meeting-videos-7.html"><a href="meeting-videos-7.html#cohort-1-7"><i class="fa fa-check"></i><b>9.3.1</b> Cohort 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>10</b> Deep Learning</a><ul>
<li class="chapter" data-level="10.1" data-path="slide-1-4.html"><a href="slide-1-4.html"><i class="fa fa-check"></i><b>10.1</b> Slide 1</a></li>
<li class="chapter" data-level="10.2" data-path="slide-2-4.html"><a href="slide-2-4.html"><i class="fa fa-check"></i><b>10.2</b> Slide 2</a></li>
<li class="chapter" data-level="10.3" data-path="meeting-videos-8.html"><a href="meeting-videos-8.html"><i class="fa fa-check"></i><b>10.3</b> Meeting Videos</a><ul>
<li class="chapter" data-level="10.3.1" data-path="meeting-videos-8.html"><a href="meeting-videos-8.html#cohort-1-8"><i class="fa fa-check"></i><b>10.3.1</b> Cohort 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="survival-analysis-and-censored-data.html"><a href="survival-analysis-and-censored-data.html"><i class="fa fa-check"></i><b>11</b> Survival Analysis and Censored Data</a><ul>
<li class="chapter" data-level="11.1" data-path="slide-1-5.html"><a href="slide-1-5.html"><i class="fa fa-check"></i><b>11.1</b> Slide 1</a></li>
<li class="chapter" data-level="11.2" data-path="slide-2-5.html"><a href="slide-2-5.html"><i class="fa fa-check"></i><b>11.2</b> Slide 2</a></li>
<li class="chapter" data-level="11.3" data-path="meeting-videos-9.html"><a href="meeting-videos-9.html"><i class="fa fa-check"></i><b>11.3</b> Meeting Videos</a><ul>
<li class="chapter" data-level="11.3.1" data-path="meeting-videos-9.html"><a href="meeting-videos-9.html#cohort-1-9"><i class="fa fa-check"></i><b>11.3.1</b> Cohort 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>12</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="12.1" data-path="slide-1-6.html"><a href="slide-1-6.html"><i class="fa fa-check"></i><b>12.1</b> Slide 1</a></li>
<li class="chapter" data-level="12.2" data-path="slide-2-6.html"><a href="slide-2-6.html"><i class="fa fa-check"></i><b>12.2</b> Slide 2</a></li>
<li class="chapter" data-level="12.3" data-path="meeting-videos-10.html"><a href="meeting-videos-10.html"><i class="fa fa-check"></i><b>12.3</b> Meeting Videos</a><ul>
<li class="chapter" data-level="12.3.1" data-path="meeting-videos-10.html"><a href="meeting-videos-10.html#cohort-1-10"><i class="fa fa-check"></i><b>12.3.1</b> Cohort 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="multiple-testing.html"><a href="multiple-testing.html"><i class="fa fa-check"></i><b>13</b> Multiple Testing</a><ul>
<li class="chapter" data-level="13.1" data-path="slide-1-7.html"><a href="slide-1-7.html"><i class="fa fa-check"></i><b>13.1</b> Slide 1</a></li>
<li class="chapter" data-level="13.2" data-path="slide-2-7.html"><a href="slide-2-7.html"><i class="fa fa-check"></i><b>13.2</b> Slide 2</a></li>
<li class="chapter" data-level="13.3" data-path="meeting-videos-11.html"><a href="meeting-videos-11.html"><i class="fa fa-check"></i><b>13.3</b> Meeting Videos</a><ul>
<li class="chapter" data-level="13.3.1" data-path="meeting-videos-11.html"><a href="meeting-videos-11.html#cohort-1-11"><i class="fa fa-check"></i><b>13.3.1</b> Cohort 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="abbreviations.html"><a href="abbreviations.html"><i class="fa fa-check"></i>Abbreviations</a></li>
<li class="chapter" data-level="" data-path="latex.html"><a href="latex.html"><i class="fa fa-check"></i>Appendix: Bookdown and LaTeX Notes</a><ul>
<li class="chapter" data-level="" data-path="markdown-highlighting.html"><a href="markdown-highlighting.html"><i class="fa fa-check"></i>Markdown highlighting</a></li>
<li class="chapter" data-level="" data-path="text-coloring.html"><a href="text-coloring.html"><i class="fa fa-check"></i>Text coloring</a></li>
<li class="chapter" data-level="" data-path="x99-4.html"><a href="x99-4.html"><i class="fa fa-check"></i>Section references</a></li>
<li class="chapter" data-level="" data-path="footnotes.html"><a href="footnotes.html"><i class="fa fa-check"></i>Footnotes</a></li>
<li class="chapter" data-level="" data-path="formatting-text.html"><a href="formatting-text.html"><i class="fa fa-check"></i>Formatting Text</a></li>
<li class="chapter" data-level="" data-path="figures.html"><a href="figures.html"><i class="fa fa-check"></i>Figures</a></li>
<li class="chapter" data-level="" data-path="displaying-formula.html"><a href="displaying-formula.html"><i class="fa fa-check"></i>Displaying Formula</a><ul>
<li class="chapter" data-level="" data-path="displaying-formula.html"><a href="displaying-formula.html#formatting"><i class="fa fa-check"></i>Formatting</a></li>
<li class="chapter" data-level="" data-path="displaying-formula.html"><a href="displaying-formula.html#symbols"><i class="fa fa-check"></i>Symbols</a></li>
<li class="chapter" data-level="" data-path="displaying-formula.html"><a href="displaying-formula.html#notation-1"><i class="fa fa-check"></i>Notation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="equations.html"><a href="equations.html"><i class="fa fa-check"></i>Equations</a><ul>
<li class="chapter" data-level="" data-path="equations.html"><a href="equations.html#basic-equation"><i class="fa fa-check"></i>Basic Equation</a></li>
<li class="chapter" data-level="" data-path="equations.html"><a href="equations.html#case-when-equation-large-curly-brace"><i class="fa fa-check"></i>Case-When Equation (Large Curly Brace)</a></li>
<li class="chapter" data-level="" data-path="equations.html"><a href="equations.html#alligned-with-underbars"><i class="fa fa-check"></i>Alligned with Underbars</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="greek-letters.html"><a href="greek-letters.html"><i class="fa fa-check"></i>Greek letters</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistical Learning Using R Book Club</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="a-comparison-of-classification-methods" class="section level2">
<h2><span class="header-section-number">4.5</span> A Comparison of Classification Methods</h2>
<p>Each of the classifiers below uses different estimates of <span class="math inline">\(f_k(x)\)</span>.</p>
<ul>
<li>linear discriminant analysis;</li>
<li>quadratic discriminant analysis;</li>
<li>naive Bayes</li>
</ul>
<div id="linear-discriminant-analysis-for-p-1" class="section level3">
<h3><span class="header-section-number">4.5.1</span> Linear Discriminant Analysis for p = 1</h3>
<ul>
<li>one predictor</li>
<li>classify an observation to the class for which <span class="math inline">\(p_k(x)\)</span> is greatest</li>
</ul>
<p><strong>Assumptions:</strong>
- we assume that <span class="math inline">\(f_k(x)\)</span> is normal or Gaussian with a classs pecific
mean and,
- a shared variance term across all K classes [<span class="math inline">\(σ^2_1 = · · · = σ^2_K\)</span> ]</p>
<p>The normal density takes the form</p>
<p><span class="math display">\[f_k(x) = \frac{1}{\sqrt{2πσk}}exp(- \frac{1}{2σ^2_k}(x- \mu_k)^2)\]</span></p>
<p>Then, the posterior probability (probability that the observation belongs to the kth class, given the predictor value for that observation) is</p>
<p><span class="math display">\[p_k(x) = \frac{π_k \frac{1}{\sqrt{2πσk}}exp(- \frac{1}{2σ^2_k}(x- \mu_k)^2)}{\sum^k_{l=1} π_l \frac{1}{\sqrt{2πσk}}exp(- \frac{1}{2σ^2_k}(x- \mu_l)^2)}\]</span></p>
<p><strong>Additional mathematical formula</strong></p>
<p>After you log and rearrange the above equation, you will the following formula. The Bayes’ classifier assign to one class if <span class="math inline">\(2x (μ_1 − μ_2) &gt; μ_1^2 − μ_2^2\)</span> and otherwise.</p>
<p><span class="math display">\[δ_k(x) = x . \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + log(π_k) \Longrightarrow {Equation \space 4.18}\]</span></p>
<p>The Bayes decision boundary is the point for which <span class="math inline">\(δ_1(x) = δ_2(x)\)</span></p>
<p><span class="math display">\[x = \frac{μ_1^2 − μ_2^2}{2(μ_1 − μ_2)} = \frac{μ_1 + μ_2}{2}\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fig4-4"></span>
<img src="images/fig4_4.jpg" alt="Left: Two one-dimensional normal density functions are shown. The dashed vertical line represents the Bayes decision boundary. Right: 20 observations were drawn from each of the two classes, and are shown as histograms. The Bayes decision boundary is again shown as a dashed vertical line. The solid vertical line represents the LDA decision boundary estimated from the training data."  />
<p class="caption">
Figure 4.3: Left: Two one-dimensional normal density functions are shown. The dashed vertical line represents the Bayes decision boundary. Right: 20 observations were drawn from each of the two classes, and are shown as histograms. The Bayes decision boundary is again shown as a dashed vertical line. The solid vertical line represents the LDA decision boundary estimated from the training data.
</p>
</div>
<p>The <strong>linear discriminant analysis (LDA)</strong> method approximates the linear
discriminant analysis Bayes classifier by plugging estimates for <span class="math inline">\(π_k\)</span>, <span class="math inline">\(μ_k\)</span>, and σ^2 into equation 4.18.</p>
<p><span class="math inline">\(\hat μ_k\)</span> is the average of all the training observations from the kth class
<span class="math display">\[\hat{\mu}_{k} = \frac{1}{n_{k}}\sum_{i: y_{i}= k} x_{i}\]</span></p>
<p><span class="math inline">\(\hat σ^2\)</span> is the weighted average of the sample variances for each of the K classes</p>
<p><span class="math display">\[\hat{\sigma}^2 = \frac{1}{n - K} \sum_{k = 1}^{K} \sum_{i: y_{i}= k} (x_{i} - \hat{\mu}_{k})^2\]</span></p>
<p>Note.
n = total number of training observations,
<span class="math inline">\(n_k\)</span> = number of training observations in the kth class</p>
<p><span class="math inline">\(π_k\)</span> is estimated from the proportion of the training observations
that belong to the kth class.</p>
<p><span class="math inline">\(π_k = \frac{n_k}{n}\)</span></p>
<p>LDA classifier assigns an observation X = x to the class for which <span class="math inline">\(δ_k(x)\)</span> is largest.</p>
<p><span class="math display">\[δ_k(x) = x . \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + log(π_k) \Longrightarrow {Equation \space 4.18} \\ \Downarrow \\ \hat δ_k(x) = x \cdot \frac{\hat \mu_k}{\hat \sigma^2} - \frac{\hat \mu_k^2}{2\hat \sigma^2} + log(\hat π_k)\]</span></p>
</div>
<div id="linear-discriminant-analysis-for-p-1-1" class="section level3">
<h3><span class="header-section-number">4.5.2</span> Linear Discriminant Analysis for p &gt; 1</h3>
<ul>
<li><p>multiple predictors; p &gt; 1 predictors</p></li>
<li><p>observations come from a multivariate Gaussian (or multivariate normal) distribution, with a <strong>class-specific mean vector</strong> and a common <strong>covariance matrix</strong>; <span class="math display">\[N(μ_k,Σ)\]</span></p></li>
</ul>
<p><strong>Assumptions: </strong>
- each individual predictor follows a one-dimensional normal distribution, with predictors having some correlation</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fig4-5"></span>
<img src="images/fig4_5.jpg" alt="Two multivariate Gaussian density functions are shown, with p = 2. Left: The two predictors are uncorrelated and it has a circular base. Var(X_1) = Var(X_2) and Cor(X_1,X_2) = 0; Right: The two variables have a correlation of 0.7 with a elliptical base"  />
<p class="caption">
Figure 4.4: Two multivariate Gaussian density functions are shown, with p = 2. Left: The two predictors are uncorrelated and it has a circular base. Var(X_1) = Var(X_2) and Cor(X_1,X_2) = 0; Right: The two variables have a correlation of 0.7 with a elliptical base
</p>
</div>
<p><span class="math inline">\(\exp\)</span>
The multivariate Gaussian density is defined as:</p>
<p><span class="math display">\[f(x) = \frac{1}{(2π)^{\frac{p}{2}}|Σ|^{\frac{1}{2}}}\exp -\frac{1}{2}(x - \mu)^T Σ^{−1}(x − μ))\]</span></p>
<p>Bayes classifier assigns an observation X = x to the class for which <span class="math display">\[δ_k(x)\]</span> is largest.</p>
<p><span class="math display">\[δ_k(x) =  x^T Σ^{−1}μ_k - \frac{1}{2}μ_k^T Σ^{−1} μ_k + log π_k \Longrightarrow vector/matrix \space version \\ δ_k(x) = x . \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + log(π_k) \Longrightarrow {Equation \space 4.18}\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fig4-6"></span>
<img src="images/fig4_6.jpg" alt="An example with three classes. The observations from each class are drawn from a multivariate Gaussian distribution with p = 2, with a class-specific mean vector and a common covariance matrix. Left: Ellipses that contain 95% of the probability for each of the three classes are shown. The dashed lines are the Bayes decision boundaries. Right: 20 observations were generated from each class, and the corresponding LDA decision boundaries are indicated using solid black lines. The Bayes decision boundaries are once again shown as dashed lines. Overall, the LDA decision boundaries are pretty close to the Bayes decision boundaries, shown again as dashed lines. The test error rates for the Bayes and LDA classifiers are 0.0746 and 0.0770, respectively."  />
<p class="caption">
Figure 4.5: An example with three classes. The observations from each class are drawn from a multivariate Gaussian distribution with p = 2, with a class-specific mean vector and a common covariance matrix. Left: Ellipses that contain 95% of the probability for each of the three classes are shown. The dashed lines are the Bayes decision boundaries. Right: 20 observations were generated from each class, and the corresponding LDA decision boundaries are indicated using solid black lines. The Bayes decision boundaries are once again shown as dashed lines. Overall, the LDA decision boundaries are pretty close to the Bayes decision boundaries, shown again as dashed lines. The test error rates for the Bayes and LDA classifiers are 0.0746 and 0.0770, respectively.
</p>
</div>
<p>All classification models have training error rate, which can be displayed with a <strong>confusion matrix</strong>.</p>
<p><strong>Caveats of error rate: </strong></p>
<ul>
<li><p>training error rates will usually be lower than test error rates, which are the real quantity of interest. The higher the ratio of parameters <em>p</em> to number of samples n, the more we expect this <em>overfitting</em> to play a role.</p></li>
<li><p>the trivial null classifier will achieve an error rate that is only a bit higher than the LDA training set error rate</p></li>
<li><p>a binary classifier such as this one can make two types of errors (Type I and II)</p></li>
<li><p>Class-specific performance <em>(sensitivity and specificity)</em> is important in certain fields (e.g., medicine)</p></li>
</ul>
<p>LDA has low sensitivity due to
1. LDA is trying to approximate the Bayes classifier, which has the lowest
total error rate out of all classifiers
2. In the process, the Bayes classifier will yield the smallest possible total number of misclassified observations, regardless of the class from which the errors stem.
3. It also uses a threshold of 50% for the posterior probability of default in order to assign an observation to the default class</p>
<p><span class="math display">\[Pr(default = Yes|X = x) &gt; 0.5. \\ Pr(default = Yes|X = x) &gt; 0.2.\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fig4-7"></span>
<img src="images/fig4_7.jpg" alt="The figure illustrates the trade-off that results from modifying the threshold value for the posterior probability of default. For the Default data set, error rates are shown as a function of the threshold value for the posterior probability that is used to perform the assignment. The black solid line displays the overall error rate. The blue dashed line represents the fraction of defaulting customers that are incorrectly classified, and the orange dotted line indicates the fraction of errors among the non-defaulting customers."  />
<p class="caption">
Figure 4.6: The figure illustrates the trade-off that results from modifying the threshold value for the posterior probability of default. For the Default data set, error rates are shown as a function of the threshold value for the posterior probability that is used to perform the assignment. The black solid line displays the overall error rate. The blue dashed line represents the fraction of defaulting customers that are incorrectly classified, and the orange dotted line indicates the fraction of errors among the non-defaulting customers.
</p>
</div>
<ul>
<li><p>As the threshold is reduced, the error rate among individuals who default decreases steadily, but the error rate among the individuals who do not default increases. The decision on the threshold must be based on <strong>domain knowledge</strong> (e.g., detailed information about the costs associated with default)</p></li>
<li><p>ROC curve is a way to illustrate the two type of errors at all possible thresholds.</p></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fig4-8"></span>
<img src="images/fig4_8.jpg" alt="The true positive rate is the sensitivity: the fraction of defaulters that are correctly identified, using a given threshold value. The false positive rate is 1-specificity: the fraction of non-defaulters that we classify incorrectly as defaulters, using that same threshold value. The ideal ROC curve hugs the top left corner, indicating a high true positive rate and a low false positive rate. The dotted line represents the “no information” classifier; this is what we would expect if student status and credit card balance are not associated with probability of default."  />
<p class="caption">
Figure 4.7: The true positive rate is the sensitivity: the fraction of defaulters that are correctly identified, using a given threshold value. The false positive rate is 1-specificity: the fraction of non-defaulters that we classify incorrectly as defaulters, using that same threshold value. The ideal ROC curve hugs the top left corner, indicating a high true positive rate and a low false positive rate. The dotted line represents the “no information” classifier; this is what we would expect if student status and credit card balance are not associated with probability of default.
</p>
</div>
<p>An ideal ROC curve will hug the top left corner, so the larger <strong>area under the ROC curve (AUC)</strong>, the better the classifier.</p>
<table class="gmisc_table" style="border-collapse: collapse; margin-top: 1em; margin-bottom: 1em;">
<thead>
<tr>
<th style="border-top: 2px solid grey;">
</th>
<th colspan="3" style="font-weight: 900; border-bottom: 1px solid grey; border-top: 2px solid grey; text-align: center;">
True class
</th>
</tr>
<tr>
<th style="border-bottom: 1px solid grey;">
</th>
<th style="font-weight: 900; border-bottom: 1px solid grey; text-align: center;">
Neg. or Null
</th>
<th style="font-weight: 900; border-bottom: 1px solid grey; text-align: center;">
Pos. or Non-null
</th>
<th style="font-weight: 900; border-bottom: 1px solid grey; text-align: center;">
Total
</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="4" style="font-weight: 900;">
Predicted class
</td>
</tr>
<tr>
<td style="text-align: left;">
   − or Null
</td>
<td style="text-align: left;">
True Neg. (TN)
</td>
<td style="text-align: center;">
False Neg. (FN)
</td>
<td style="text-align: right;">
N∗
</td>
</tr>
<tr>
<td style="text-align: left;">
   + or Non-null
</td>
<td style="text-align: left;">
False Pos. (FP)
</td>
<td style="text-align: center;">
True Pos. (TP)
</td>
<td style="text-align: right;">
P∗
</td>
</tr>
<tr>
<td style="border-bottom: 2px solid grey; text-align: left;">
  Total
</td>
<td style="border-bottom: 2px solid grey; text-align: left;">
N
</td>
<td style="border-bottom: 2px solid grey; text-align: center;">
P
</td>
<td style="border-bottom: 2px solid grey; text-align: right;">
</td>
</tr>
</tbody>
</table>
<p>Important measures for classification and diagnostic testing:</p>
<ul>
<li><p><strong>False Positive rate (FP/N)</strong> <span class="math inline">\(\Longrightarrow\)</span> Type I error, 1−Specificity</p></li>
<li><p><strong>True Positive rate (TP/P)</strong> <span class="math inline">\(\Longrightarrow\)</span> 1−Type II error, power, sensitivity, recall</p></li>
<li><p><strong>Pos. Predicted value (TP/P∗)</strong> <span class="math inline">\(\Longrightarrow\)</span> Precision, 1−false discovery proportion</p></li>
<li><p><strong>Neg. Predicted value (TN/N∗)</strong></p></li>
</ul>
</div>
<div id="quadratic-discriminant-analysis-qda" class="section level3">
<h3><span class="header-section-number">4.5.3</span> Quadratic Discriminant Analysis (QDA)</h3>
<ul>
<li><p>Assumptions similar to LDA, in which observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to perform prediction</p></li>
<li><p>QDA assumes that each class has its own covariance matrix</p></li>
</ul>
<p><span class="math display">\[X ∼ N(μ_k,Σ_k) \Longrightarrow {Σ_k is \space covariance \space matrix \space for \space the \space kth \space class}\]</span></p>
<p><strong>Bayes classifier</strong></p>
<p><span class="math display">\[δ_k(x) = - \frac{1}{2}(x - \mu_k)^T Σ_k^{−1}(x - \mu_k) - \frac{1}{2}log|Σ_k| + log(π_k) \\ \Downarrow \\ δ_k(x) =  - \frac{1}{2}x^T Σ_k^{−1}x - x^T Σ_k^{−1} \mu_k - \frac{1}{2}μ_k^T Σ_k^{−1} μ_k - \frac{1}{2}log|Σ_k| + log π_k\]</span></p>
<p>QDA classifier involves plugging estimates for <strong><span class="math inline">\(Σ_k\)</span>, <span class="math inline">\(μ_k\)</span>, and <span class="math inline">\(π_k\)</span></strong> into the above equation, and then assigning an observation X = x to the class for which this quantity is <strong>largest</strong>.</p>
<p>The quantity x appears as a quadratic function, hence the name.</p>
<p><br>
<strong>Why the LDA to QDA is preferred or vice-versa?</strong>
<br>
1. <strong>Bias-variance trade-off</strong>
<br>
- Pro LDA: LDA assumes that the K classes share a common covariance matrix and the quantity X becomes linear, which means there are <span class="math inline">\(K_p\)</span> linear coefficients to estimate.LDA is a much less flexible classifier than QDA, and so has substantially <em>lower variance</em>; improved prediction performance.</p>
<ul>
<li><p>Con LDA: If the assumption K classes share a common covariance matrix is badly off, LDA can suffer from <em>high bias</em></p></li>
<li><p>Conclusion: Use LDA when there is a few training observations; use QDA when the training set is very large or common covariance matrix is untennable.</p></li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fig4-9"></span>
<img src="images/fig4_9.jpg" alt="Left: The Bayes (purple dashed), LDA (black dotted), and QDA (green solid) decision boundaries for a two-class problem with Σ1 = Σ2. The shading indicates the QDA decision rule. Since the Bayes decision boundary is linear, it is more accurately approximated by LDA than by QDA. Right: Details are as given in the left-hand panel, except that Σ1 ̸= Σ2. Since the Bayes decision boundary is non-linear, it is more accurately approximated by QDA than by LDA."  />
<p class="caption">
Figure 4.8: Left: The Bayes (purple dashed), LDA (black dotted), and QDA (green solid) decision boundaries for a two-class problem with Σ1 = Σ2. The shading indicates the QDA decision rule. Since the Bayes decision boundary is linear, it is more accurately approximated by LDA than by QDA. Right: Details are as given in the left-hand panel, except that Σ1 ̸= Σ2. Since the Bayes decision boundary is non-linear, it is more accurately approximated by QDA than by LDA.
</p>
</div>
</div>
<div id="naive-bayes" class="section level3">
<h3><span class="header-section-number">4.5.4</span> Naive Bayes</h3>
<ul>
<li>Estimating a p-dimensional density function is challenging; naive bayes make a different assumption than LDA and QDA.</li>
<li>an alternative to LDA that does not assume normally distributed
predictors</li>
</ul>
<p><span class="math display">\[f_k(x) = f_{k1}(x_1) × f_{k2}(x_2)×· · ·×f{k_p}(x_p),\]</span>
where <span class="math inline">\(f_{kj}\)</span> is the density function of the jth predictor among observations in the kth class</p>
<p><em>Within the kth class, the p predictors are independent.</em></p>
<p><strong>Why naive Bayes is better/powerful?</strong></p>
<ol style="list-style-type: decimal">
<li><p>By assuming that the p covariates are independent within each class, we assumed that there is no association between the predictors! When estimating a p-dimensional density function, it is difficult to calculate the <em>marginal distribution</em> of each predictor and <em>joint distribution</em> of the predictors.</p></li>
<li><p>Although p covariates might not be independent within each class, it is convenient and we obtain pretty decent results when the n is small, p is large.</p></li>
<li><p>It reduces variance, though it has some bias (Bias-variance trade-off)</p></li>
</ol>
<p><strong>Options to estimate the one-dimensional density function fkj using training data</strong></p>
<ol style="list-style-type: decimal">
<li><p>[For Quantitative <span class="math inline">\(X_j\)</span>] -&gt; We assume <span class="math inline">\(X_j |Y = k ∼ N(μ_{jk},σ_{jk}^2)\)</span>, where within each class, the jth predictor is drawn from a (univariate) normal distribution. It is <strong>QDA-like with diagonal class-specific covariance matrix</strong></p></li>
<li><p>[For Quantitative <span class="math inline">\(X_j\)</span>] -&gt; Use a <em>non-parametric estimate</em> for <span class="math inline">\(f_{kj}\)</span>. First, a histogram for the within-class observations and then estimate <span class="math inline">\(f_{kj}(x_j)\)</span>. Or else, use <strong>kernel density estimator</strong>.</p></li>
<li><p>[For Qualitative <span class="math inline">\(X_j\)</span>] -&gt;Count the proportion of training observations for the jth predictor corresponding to each class.</p></li>
</ol>
<p>Note: Fixing the threshold, the Naive Bayes has a higher error rate than LDA, but better prediction (higher sensitivity).</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="generative-models-for-classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary-of-the-classification-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/r4ds/bookclub-islr/edit/main/04.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
